#1. ARIMA(4,1,4)
# Importar librerías necesarias
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from statsmodels.tsa.stattools import adfuller
from google.colab import files

# Cargar el archivo desde Google Colab
uploaded = files.upload()  # Sube manualmente tu archivo

# Leer el archivo Excel
file_name = list(uploaded.keys())[0]  # Obtener el nombre del archivo subido
df = pd.read_excel(file_name)

# Convertir la columna 'fecha' a formato datetime y extraer solo los años
df["fecha"] = pd.to_datetime(df["fecha"], errors="coerce")

# Aplicar logaritmo a la tasa de paro
df["log_tasa_paro"] = np.log(df["tasa_paro"])

# Graficar antes y después de aplicar logaritmos
plt.figure(figsize=(12, 5))

# Gráfica de la tasa de paro original
plt.subplot(1, 2, 1)
plt.plot(df["fecha"], df["tasa_paro"], marker="o", linestyle="-", label="Tasa de Paro")
plt.xlabel("Fecha")
plt.ylabel("Tasa de Paro")
plt.title("Tasa de Paro Original")
plt.legend()

# Configurar el eje x para que solo muestre años
plt.gca().xaxis.set_major_locator(mdates.YearLocator())  # Mostrar solo años
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter("%Y"))  # Formato de años
plt.xticks(rotation=45)

# Gráfica de la tasa de paro en logaritmo
plt.subplot(1, 2, 2)
plt.plot(df["fecha"], df["log_tasa_paro"], marker="o", linestyle="-", color="red", label="Log(Tasa de Paro)")
plt.xlabel("Fecha")
plt.ylabel("Log(Tasa de Paro)")
plt.title("Tasa de Paro Transformada")
plt.legend()

# Configurar el eje x para que solo muestre años
plt.gca().xaxis.set_major_locator(mdates.YearLocator())  # Mostrar solo años
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter("%Y"))  # Formato de años
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

# Prueba de Dickey-Fuller sobre la serie logarítmica
adf_test = adfuller(df["log_tasa_paro"])
print("Resultados de la prueba de Dickey-Fuller:")
print(f"Estadístico ADF: {adf_test[0]}")
print(f"p-value: {adf_test[1]}")
print("Valores críticos:")
for key, value in adf_test[4].items():
    print(f"\t{key}: {value}")

if adf_test[1] < 0.05:
    print("\nLa serie es estacionaria (rechazamos H0).")
else:
    print("\nLa serie NO es estacionaria (no podemos rechazar H0).")

# Asegurar que la columna de fecha esté en formato datetime
df["fecha"] = pd.to_datetime(df["fecha"], errors="coerce")

# Aplicar una diferenciación a la serie logarítmica
df["diff_log_tasa_paro"] = df["log_tasa_paro"].diff()

# Graficar después de la diferenciación
plt.figure(figsize=(10, 5))

# Graficar la serie diferenciada
plt.plot(df["fecha"], df["diff_log_tasa_paro"], marker="o", linestyle="-", color="green", label="Diferenciada Log(Tasa de Paro)")
plt.xlabel("Fecha")
plt.ylabel("Diferenciada Log(Tasa de Paro)")
plt.title("Serie Logarítmica Diferenciada de la Tasa de Paro")
plt.legend()

# Configurar el eje x para mostrar solo los años correctamente
import matplotlib.dates as mdates
plt.gca().xaxis.set_major_locator(mdates.YearLocator())  # Mostrar solo años
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter("%Y"))  # Formato de años
plt.xticks(rotation=45)

plt.show()

# Prueba de Dickey-Fuller sobre la serie diferenciada
adf_test_diff = adfuller(df["diff_log_tasa_paro"].dropna())  # Se eliminan valores NaN por la diferenciación
print("Resultados de la prueba de Dickey-Fuller después de la diferenciación:")
print(f"Estadístico ADF: {adf_test_diff[0]}")
print(f"p-value: {adf_test_diff[1]}")
print("Valores críticos:")
for key, value in adf_test_diff[4].items():
    print(f"\t{key}: {value}")

if adf_test_diff[1] < 0.05:
    print("\n La serie es estacionaria después de la diferenciación (rechazamos H0).")
else:
    print("\n La serie NO es estacionaria después de la diferenciación (no podemos rechazar H0).")
# Importar librerías necesarias para la FAC y FACP
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Configurar la figura para mostrar la FAC y la FACP
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Gráfico de la Función de Autocorrelación (FAC)
plot_acf(df["diff_log_tasa_paro"].dropna(), ax=axes[0], lags=20)
axes[0].set_title("Función de Autocorrelación (FAC)")

# Gráfico de la Función de Autocorrelación Parcial (FACP)
plot_pacf(df["diff_log_tasa_paro"].dropna(), ax=axes[1], lags=20)
axes[1].set_title("Función de Autocorrelación Parcial (FACP)")

plt.tight_layout()
plt.show()
# Importar librerías necesarias
import warnings
import itertools
import statsmodels.api as sm

# Definir los valores posibles para p, d, q
p = range(0, 5)  # Valores para AR (Auto-Regresivo)
d = range(0, 3)  # Valores para Diferenciación
q = range(0, 5)  # Valores para MA (Media Móvil)

# Generar todas las combinaciones posibles de parámetros (p, d, q)
parametros_arima = list(itertools.product(p, d, q))

# Crear variables para almacenar el mejor modelo y su AIC
mejor_aic = float("inf")
mejor_orden = None
mejor_modelo = None

# Desactivar warnings para evitar mensajes innecesarios
warnings.filterwarnings("ignore")

# Iterar sobre todas las combinaciones de parámetros para encontrar la mejor
for param in parametros_arima:
    try:
        modelo = sm.tsa.ARIMA(df["log_tasa_paro"].dropna(), order=param)
        resultado = modelo.fit()
        if resultado.aic < mejor_aic:
            mejor_aic = resultado.aic
            mejor_orden = param
            mejor_modelo = resultado
    except:
        continue  # Si el modelo no converge, pasa al siguiente

# Mostrar la mejor combinación de parámetros y su AIC
print(f"Mejor combinación ARIMA: {mejor_orden} con AIC: {mejor_aic}")
# Verificar si se ha encontrado un modelo ARIMA óptimo
if mejor_modelo:
    # Extraer coeficientes del modelo
    coeficientes = mejor_modelo.params

    # Crear un DataFrame con los coeficientes
    coef_df = pd.DataFrame(coeficientes, columns=["Valor"])
    coef_df.index.name = "Parámetro"

    # Mostrar la tabla de coeficientes
    from IPython.display import display
    print(f"Coeficientes del modelo ARIMA {mejor_orden}:")
    display(coef_df)
else:
    print("No se encontró un modelo ARIMA válido.")
from statsmodels.stats.diagnostic import acorr_ljungbox
import matplotlib.dates as mdates

# Asegurar que la columna de fecha esté en formato datetime
df["fecha"] = pd.to_datetime(df["fecha"], errors="coerce")

# Calcular los residuos del mejor modelo ARIMA
residuos = mejor_modelo.resid

# Calcular la media de los errores
media_errores = residuos.mean()

# Aplicar la prueba de Ljung-Box para autocorrelación en los residuos
ljung_box = acorr_ljungbox(residuos, lags=[10], return_df=True)

# Crear una tabla con los resultados
resultados_df = pd.DataFrame({
    "Métrica": ["Media de los errores", "Ljung-Box p-valor (lag 10)"],
    "Valor": [media_errores, ljung_box["lb_pvalue"].values[0]]
})

# Mostrar la tabla de resultados
print("Resultados de la evaluación del modelo:")
display(resultados_df)

# Ajustar la longitud de la fecha para coincidir con los residuos
fechas_ajustadas = df["fecha"].iloc[-len(residuos):]  # Tomar las últimas fechas que coincidan con los residuos

# Graficar los residuos en el tiempo
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(fechas_ajustadas, residuos, marker="o", linestyle="-", color="blue")
plt.axhline(y=0, color="red", linestyle="--")
plt.xlabel("Fecha")
plt.ylabel("Residuos")
plt.title("Residuos del Modelo en el Tiempo")

# Configurar el eje x para mostrar solo los años correctamente
plt.gca().xaxis.set_major_locator(mdates.YearLocator())  # Mostrar solo años
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter("%Y"))  # Formato de años
plt.xticks(rotation=45)

# Histograma de los residuos
plt.subplot(1, 2, 2)
plt.hist(residuos, bins=20, color="purple", alpha=0.7, edgecolor="black")
plt.xlabel("Valor de los Residuos")
plt.ylabel("Frecuencia")
plt.title("Histograma de los Residuos")

plt.tight_layout()
plt.show()
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np
import matplotlib.dates as mdates

# Asegurar que la columna de fecha esté en formato datetime
df["fecha"] = pd.to_datetime(df["fecha"], errors="coerce")

# Dividir los datos en entrenamiento (80%) y test (20%)
train_size = int(len(df) * 0.8)
train, test = df["tasa_paro"][:train_size], df["tasa_paro"][train_size:]

# Ajustar el modelo ARIMA(4,1,4) en los datos de entrenamiento
modelo_arima = ARIMA(train, order=(4,1,4))
modelo_ajustado = modelo_arima.fit()

# Predecir valores en el conjunto de entrenamiento
pred_train = modelo_ajustado.fittedvalues

# Predecir valores en el conjunto de prueba
pred_test = modelo_ajustado.forecast(steps=len(test))

# Calcular métricas de error
rmse_train = np.sqrt(mean_squared_error(train[1:], pred_train[1:]))  # Ignorar primer valor NaN
mae_train = mean_absolute_error(train[1:], pred_train[1:])

rmse_test = np.sqrt(mean_squared_error(test, pred_test))
mae_test = mean_absolute_error(test, pred_test)

# Crear una tabla con los resultados
resultados_error = pd.DataFrame({
    "Métrica": ["RMSE Entrenamiento", "MAE Entrenamiento", "RMSE Test", "MAE Test"],
    "Valor": [rmse_train, mae_train, rmse_test, mae_test]
})

# Mostrar la tabla de métricas
print("Resultados del Modelo ARIMA(4,1,4):")
display(resultados_error)

# Graficar los resultados
plt.figure(figsize=(12, 6))

# Serie original vs. predicción en entrenamiento y test
plt.plot(df["fecha"], df["tasa_paro"], label="Tasa de Paro Real", color="black")
plt.plot(df["fecha"][:train_size], pred_train, label="Predicción Entrenamiento", color="blue")
plt.plot(df["fecha"][train_size:], pred_test, label="Predicción Test", color="red")
plt.axvline(df["fecha"].iloc[train_size], color="gray", linestyle="--", label="Inicio Test")

# Configurar el eje x para mostrar solo los años correctamente
plt.gca().xaxis.set_major_locator(mdates.YearLocator())  # Mostrar solo años
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter("%Y"))  # Formato de años
plt.xticks(rotation=45)

plt.xlabel("Fecha")
plt.ylabel("Tasa de Paro (%)")
plt.title("Modelo ARIMA(4,1,4) - Predicción vs Realidad")
plt.legend()

# Eliminar cuadrícula del gráfico
plt.grid(False)

plt.show()
print(modelo_ajustado.summary())
from scipy.stats import kstest, norm

# Obtener los residuos del modelo
residuos = modelo_ajustado.resid

# Normalizar los residuos (restar la media y dividir por la desviación estándar)
residuos_norm = (residuos - np.mean(residuos)) / np.std(residuos)

# Aplicar el test de Kolmogorov-Smirnov comparando con una distribución normal
ks_stat, p_value = kstest(residuos_norm, 'norm')

# Crear una tabla con los resultados
resultado_ks = pd.DataFrame({
    "Métrica": ["Estadístico KS", "p-valor"],
    "Valor": [ks_stat, p_value]
})

# Mostrar la tabla con los resultados
print("Resultados del Test de Kolmogorov-Smirnov:")
display(resultado_ks)

# Interpretación del resultado
if p_value > 0.05:
    print("\nNo se rechaza la hipótesis nula: los residuos siguen una distribución normal.")
else:
    print("\nSe rechaza la hipótesis nula: los residuos NO siguen una distribución normal.")
import matplotlib.dates as mdates
from statsmodels.tsa.arima.model import ARIMA

# Convertir la columna de fecha a tipo datetime para evitar errores en la gráfica
df["fecha"] = pd.to_datetime(df["fecha"], errors='coerce')

# Ajustar el modelo ARIMA(4,1,4) sobre la serie original (sin logaritmos)
modelo_arima_original = ARIMA(df["tasa_paro"], order=(4,1,4))
modelo_ajustado_original = modelo_arima_original.fit()

# Definir la cantidad de pasos a predecir (8 trimestres para 2025 y 2026)
pasos_prediccion = 8

# Hacer la predicción
pred_futuro_original = modelo_ajustado_original.forecast(steps=pasos_prediccion)

# Crear fechas futuras (asumiendo que los datos son trimestrales)
ult_fecha = df["fecha"].iloc[-1]  # Última fecha registrada
fechas_futuras = pd.date_range(start=ult_fecha, periods=pasos_prediccion+1, freq="Q")[1:]  # Fechas futuras trimestrales

# Crear DataFrame con las predicciones
predicciones_df_original = pd.DataFrame({"Fecha": fechas_futuras, "Predicción Tasa de Paro": pred_futuro_original})

# Mostrar la tabla con las predicciones
print("Predicciones de la Tasa de Paro para 2025 y 2026 (sin logaritmos):")
display(predicciones_df_original)

# Graficar la serie original junto con las predicciones futuras
plt.figure(figsize=(12, 6))

# Graficar la serie real
plt.plot(df["fecha"], df["tasa_paro"], label="Tasa de Paro Real", color="black")

# Graficar la predicción
plt.plot(predicciones_df_original["Fecha"], predicciones_df_original["Predicción Tasa de Paro"],
         label="Predicción 2025-2026", color="red", linestyle="dashed", marker="o")

# Ajustar la gráfica
plt.axvline(x=df["fecha"].iloc[-1], color="gray", linestyle="--", label="Inicio Predicción")

# Configurar el eje x para mostrar solo los años correctamente
plt.gca().xaxis.set_major_locator(mdates.YearLocator())  # Mostrar solo años
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter("%Y"))  # Formato de años
plt.xticks(rotation=45)

plt.xlabel("Fecha")
plt.ylabel("Tasa de Paro (%)")
plt.title("Predicción de la Tasa de Paro para 2025 y 2026 con ARIMA(4,1,4) (sin logaritmos)")
plt.legend()

# Eliminar cuadrícula del gráfico
plt.grid(False)

plt.show()
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
import statsmodels.api as sm
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.stats.diagnostic import acorr_ljungbox
from statsmodels.tsa.stattools import adfuller
from scipy.stats import kstest, jarque_bera

# Obtener los residuos del modelo ARIMA(4,1,4)
residuos = modelo_ajustado.resid

# Crear una figura para las visualizaciones
fig, axes = plt.subplots(2, 3, figsize=(18, 10))

# 1️ Gráfico de los residuos en el tiempo
axes[0, 0].plot(residuos, marker="o", linestyle="-", color="blue")
axes[0, 0].axhline(y=0, color="red", linestyle="--")
axes[0, 0].set_xlabel("Fecha")
axes[0, 0].set_ylabel("Residuos")
axes[0, 0].set_title("Residuos del Modelo en el Tiempo")

# 2️ Histograma de los residuos
axes[0, 1].hist(residuos, bins=20, color="purple", alpha=0.7, edgecolor="black")
axes[0, 1].set_xlabel("Valor de los Residuos")
axes[0, 1].set_ylabel("Frecuencia")
axes[0, 1].set_title("Histograma de los Residuos")

# 3️ Q-Q Plot para verificar normalidad
sm.qqplot(residuos, line="s", ax=axes[0, 2])
axes[0, 2].set_title("Q-Q Plot de los Residuos")

# 4️ Autocorrelación de los residuos (FAC)
plot_acf(residuos, ax=axes[1, 0], lags=20)
axes[1, 0].set_title("Función de Autocorrelación (FAC) de los Residuos")

# 5️ Autocorrelación parcial de los residuos (FACP)
plot_pacf(residuos, ax=axes[1, 1], lags=20)
axes[1, 1].set_title("Función de Autocorrelación Parcial (FACP) de los Residuos")

plt.tight_layout()
plt.show()

# Pruebas estadísticas

# 6️ Prueba de Dickey-Fuller para estacionariedad
adf_test = adfuller(residuos)
print("\nResultados de la prueba de Dickey-Fuller:")
print(f"Estadístico ADF: {adf_test[0]}")
print(f"p-value: {adf_test[1]}")
print("Valores críticos:")
for key, value in adf_test[4].items():
    print(f"\t{key}: {value}")

if adf_test[1] < 0.05:
    print("\n Los residuos son estacionarios (rechazamos H0).")
else:
    print("\n Los residuos NO son estacionarios (no podemos rechazar H0).")

# 7️ Prueba de Ljung-Box para autocorrelación en los residuos
ljung_box = acorr_ljungbox(residuos, lags=[10], return_df=True)
print("\nResultados de la prueba de Ljung-Box:")
print(ljung_box)

if ljung_box["lb_pvalue"].values[0] > 0.05:
    print("\n No hay autocorrelación significativa en los residuos (p > 0.05).")
else:
    print("\n Hay autocorrelación significativa en los residuos (p ≤ 0.05).")

# 8️ Prueba de Kolmogorov-Smirnov (KS Test) para normalidad
ks_stat, p_value_ks = kstest((residuos - residuos.mean()) / residuos.std(), 'norm')
print("\nResultados del Test de Kolmogorov-Smirnov:")
print(f"Estadístico KS: {ks_stat}")
print(f"p-value: {p_value_ks}")

if p_value_ks > 0.05:
    print("\n Los residuos siguen una distribución normal (p > 0.05).")
else:
    print("\n Los residuos NO siguen una distribución normal (p ≤ 0.05).")

# 9️⃣ Prueba de Jarque-Bera para normalidad basada en asimetría y curtosis
jb_stat, p_value_jb = jarque_bera(residuos)
print("\nResultados de la prueba de Jarque-Bera:")
print(f"Estadístico JB: {jb_stat}")
print(f"p-value: {p_value_jb}")

if p_value_jb > 0.05:
    print("\n Los residuos tienen una distribución normal según Jarque-Bera (p > 0.05).")
else:
    print("\n Los residuos NO tienen una distribución normal según Jarque-Bera (p ≤ 0.05).")
#1.2 SARIMA (1,1,0)(1,0,1)[4]
# Importar librerías necesarias
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.tsa.statespace.sarimax import SARIMAX
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from statsmodels.stats.diagnostic import acorr_ljungbox

# Cargar el archivo
file_path = "/content/tabla_paro_ordenada.xlsx"
df = pd.read_excel(file_path)

# Convertir la columna 'fecha' en índice de serie temporal
df['fecha'] = pd.to_datetime(df['fecha'])
df.set_index('fecha', inplace=True)

# Extraer la serie de tasa de paro y aplicar logaritmo
serie_tasa_paro = np.log(df['tasa_paro'])

# Aplicar el modelo ARIMA(1,1,2) x SARIMA(0,0,1,4) (suponiendo estacionalidad trimestral)
modelo = SARIMAX(serie_tasa_paro, order=(1,1,0), seasonal_order=(1,0,1,4))
resultado = modelo.fit()

# Análisis de residuos
residuos = resultado.resid
plt.figure(figsize=(12,6))
plt.plot(residuos.index, residuos, label='Residuos')
plt.axhline(y=0, color='black', linestyle='--')
plt.title('Residuos del Modelo SARIMA')
plt.legend()
plt.grid()
plt.show()

# Prueba de autocorrelación en los residuos
ljung_box_test = acorr_ljungbox(residuos, lags=[10], boxpierce=False)
print("Prueba de Ljung-Box para autocorrelación en los residuos:")
print(ljung_box_test)

# Predicción para el periodo 2020-2024
inicio_pred = '2020-01-01'
fin_pred = '2024-12-31'
predicciones = resultado.get_prediction(start=inicio_pred, end=fin_pred, dynamic=False)
pred_media = np.exp(predicciones.predicted_mean)  # Revertir logaritmo

# Ajustar longitud de valores reales para comparación
valores_reales = np.exp(serie_tasa_paro[inicio_pred:fin_pred])  # Revertir logaritmo
valores_predichos = pred_media.loc[valores_reales.index]  # Asegurar misma longitud

# Graficar la comparación entre predicción y valores reales (2020-2024)
plt.figure(figsize=(12, 6))
plt.plot(valores_reales, label="Tasa de Paro Real", color="blue")
plt.plot(valores_predichos, label="Predicción ARIMA x SARIMA", color="red", linestyle="dashed")
plt.axvline(x=pd.Timestamp('2020-01-01'), color='black', linestyle='--', label='Inicio de Predicción')

plt.title("Comparación entre Predicción y Datos Reales (2020-2024)")
plt.xlabel("Fecha")
plt.ylabel("Tasa de Paro")
plt.legend()
plt.grid(True)
plt.show()

# Calcular  RMSE y MAE en el conjunto de test

rmse = np.sqrt(mean_squared_error(valores_reales, valores_predichos))
mae = mean_absolute_error(valores_reales, valores_predichos)


print(f"RMSE en test: {rmse:.4f}")
print(f"MAE en test: {mae:.4f}")

# Evaluación en el conjunto de entrenamiento
pred_train = np.exp(resultado.fittedvalues)
real_train = np.exp(serie_tasa_paro[:pred_train.index[-1]])  # Ajustar longitud

rmse_train = np.sqrt(mean_squared_error(real_train, pred_train))
mae_train = mean_absolute_error(real_train, pred_train)

print(f"RMSE en train: {rmse_train:.4f}")
print(f"MAE en train: {mae_train:.4f}")

# Tabla de métricas
tabla_metricas = pd.DataFrame({
    'RMSE': [rmse_train, rmse],
    'MAE': [mae_train, mae]
}, index=['Train', 'Test'])
print(tabla_metricas)

# Predicción para 2025-2026
predicciones_nuevas = resultado.get_forecast(steps=8)
pred_media_nueva = np.exp(predicciones_nuevas.predicted_mean)  # Revertir logaritmo
pred_conf_int = np.exp(predicciones_nuevas.conf_int())  # Revertir logaritmo en intervalos de confianza

# Graficar predicción para 2025-2026
plt.figure(figsize=(12, 6))
plt.plot(valores_reales, label="Tasa de Paro Real", color="blue")
plt.plot(pred_media_nueva, label="Predicción 2025-2026", color="green", linestyle="dashed")
plt.fill_between(pred_conf_int.index, pred_conf_int.iloc[:, 0], pred_conf_int.iloc[:, 1],
                 color='green', alpha=0.2, label="Intervalo de Confianza")
plt.axvline(x=pd.Timestamp('2025-01-01'), color='black', linestyle='--', label='Inicio de Predicción')

plt.title("Predicción de la Tasa de Paro para 2025-2026")
plt.xlabel("Fecha")
plt.ylabel("Tasa de Paro")
plt.legend()
plt.grid(True)
plt.show()

# Mostrar la tabla de predicciones
pred_df = pd.DataFrame({
    'Predicción': pred_media_nueva,
    'Límite Inferior': pred_conf_int.iloc[:, 0],
    'Límite Superior': pred_conf_int.iloc[:, 1]
})
print(pred_df)
#2. ARIMAX
# Instalar dependencias necesarias en Google Colab
!pip install openpyxl

# Importar librerías necesarias
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab import files

# Cargar el archivo desde Google Drive o localmente en Colab
uploaded = files.upload()
file_name = list(uploaded.keys())[0]  # Obtener el nombre del archivo subido
df = pd.read_excel(file_name, sheet_name="TABLA")

# Eliminar columnas no numéricas si las hay
df_numeric = df.select_dtypes(include=['number'])

# Calcular la matriz de correlación
correlation_matrix = df_numeric.corr()

# Visualizar la matriz de correlación con un heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Matriz de Correlación")
plt.show()

# Mostrar la matriz de correlación como tabla
print("\nMatriz de Correlación:")
print(correlation_matrix)
# Librerías necesarias
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import grangercausalitytests
from google.colab import files

# Subir archivo desde la computadora
uploaded = files.upload()

# Cargar el archivo Excel (ajusta el nombre si es diferente)
file_name = "tabla_paro_ordenada.xlsx"
xls = pd.ExcelFile(file_name)

# Verificar hojas disponibles
print("Hojas disponibles:", xls.sheet_names)

# Cargar la hoja de datos
df = pd.read_excel(xls, sheet_name='TABLA')

# Convertir la columna 'fecha' a formato datetime correctamente
df['fecha'] = pd.PeriodIndex(df['fecha'], freq='Q').to_timestamp()

# Establecer como índice
df.set_index('fecha', inplace=True)

# Calcular la correlación de cada variable con la tasa de paro
correlations = df.corr()['tasa_paro'].drop('tasa_paro')

# Graficar la correlación de cada variable con la tasa de paro
fig, axes = plt.subplots(nrows=len(correlations), ncols=1, figsize=(8, 4 * len(correlations)))

for ax, (variable, corr) in zip(axes, correlations.items()):
    sns.regplot(x=df[variable], y=df['tasa_paro'], ax=ax, scatter_kws={'alpha':0.5})
    ax.set_title(f'Correlación de Tasa de Paro con {variable}: {corr:.2f}')
    ax.set_xlabel(variable)
    ax.set_ylabel('Tasa de Paro')

plt.tight_layout()
plt.show()

# Realizar test de causalidad de Granger (máximo 4 rezagos)
max_lags = 4
granger_results = {}
for variable in df.columns:
    if variable != 'tasa_paro':
        test_result = grangercausalitytests(df[['tasa_paro', variable]].dropna(), max_lags, verbose=False)
        p_values = [test_result[i+1][0]['ssr_ftest'][1] for i in range(max_lags)]
        min_p_value = min(p_values)  # Tomamos el mínimo p-valor para determinar causalidad
        granger_results[variable] = min_p_value

# Convertir resultados en DataFrame y mostrar
granger_df = pd.DataFrame(list(granger_results.items()), columns=['Variable', 'Min P-Value'])
granger_df.sort_values(by='Min P-Value', inplace=True)

# Mostrar los resultados de causalidad de Granger
print("\nResultados del Test de Causalidad de Granger:")
print(granger_df)
# Librerías necesarias
import pandas as pd
import numpy as np
import itertools
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
import warnings
warnings.filterwarnings("ignore")

# Cargar datos
file_name = "/content/tabla_paro_diferenciada_tfg.xlsx"
xls = pd.ExcelFile(file_name)
df = pd.read_excel(xls, sheet_name='Hoja1')

# Convertir la columna 'fecha' a datetime y establecer como índice
df['fecha'] = pd.PeriodIndex(df['fecha'], freq='Q').to_timestamp()
df.set_index('fecha', inplace=True)

# Variables para el ARIMAX
y = df['tasa_paro']  # Variable dependiente
exog_vars = ['coste_laboral', 'empresas_creadas', 'salario_medio', 'PIB']  # Variables exógenas seleccionadas
X = df[exog_vars]  # Matriz de variables exógenas

# Comprobación de estacionariedad
result = adfuller(y)
if result[1] > 0.05:  # Si p-valor > 0.05, diferenciamos para hacerlo estacionario
    y = y.diff().dropna()
    X = X.iloc[1:]  # Ajustar X para que tenga la misma cantidad de datos que y diferenciada

# Definir combinaciones de parámetros ARIMA (p, d, q)
p = range(0, 3)  # Autoregresivos
d = range(0, 2)  # Diferenciación
q = range(0, 3)  # Media móvil

# Generar todas las combinaciones posibles
param_combinations = list(itertools.product(p, d, q))

# Ajustar modelos ARIMAX y encontrar el mejor (según AIC)
best_aic = float("inf")
best_order = None
best_model = None

for order in param_combinations:
    try:
        model = sm.tsa.ARIMA(y, order=order, exog=X).fit()
        aic = model.aic
        if aic < best_aic:
            best_aic = aic
            best_order = order
            best_model = model
    except:
        continue

# Mostrar el mejor modelo
print(f"\nMejor modelo ARIMAX encontrado: ARIMA{best_order} con AIC = {best_aic:.2f}")
print(best_model.summary())
# Librerías necesarias
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error
import warnings
warnings.filterwarnings("ignore")

# Cargar datos
file_name = "/content/tabla_paro_ordenada.xlsx"
xls = pd.ExcelFile(file_name)
df = pd.read_excel(xls, sheet_name='TABLA')

# Convertir la columna 'fecha' a datetime y establecer como índice
df['fecha'] = pd.PeriodIndex(df['fecha'], freq='Q').to_timestamp()
df.set_index('fecha', inplace=True)

# Variables para el ARIMAX
y = df['tasa_paro']  # Variable dependiente
exog_vars = ['coste_laboral', 'empresas_creadas', 'salario_medio']  # Variables exógenas seleccionadas
X = df[exog_vars]  # Matriz de variables exógenas

# Comprobación de estacionariedad
result = adfuller(y)
if result[1] > 0.05:  # Si p-valor > 0.05, diferenciamos para hacerlo estacionario
    y = y.diff().dropna()
    X = X.iloc[1:]  # Ajustar X para que tenga la misma cantidad de datos que y diferenciada

# Dividir los datos en entrenamiento (80%) y prueba (20%)
train_size = int(len(y) * 0.8)
y_train, y_test = y[:train_size], y[train_size:]
X_train, X_test = X[:train_size], X[train_size:]

# Mejor modelo ARIMAX basado en la selección previa
best_order = (3, 1, 1)  # Sustituye con el mejor modelo encontrado en el paso anterior

# Ajustar el modelo con los datos de entrenamiento
model = sm.tsa.ARIMA(y_train, order=best_order, exog=X_train).fit()

# Hacer predicciones
y_pred = model.predict(start=len(y_train), end=len(y_train) + len(y_test) - 1, exog=X_test)

# Evaluación del rendimiento del modelo
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mape = mean_absolute_percentage_error(y_test, y_pred) * 100

# Mostrar resultados
print(f"\nEvaluación del Modelo ARIMAX {best_order}:")
print(f"RMSE: {rmse:.2f}")
print(f"MAPE: {mape:.2f}%")

# Graficar los resultados
import matplotlib.pyplot as plt

plt.figure(figsize=(10,5))
plt.plot(y_train, label='Entrenamiento')
plt.plot(y_test, label='Real (Test)', color='black')
plt.plot(y_pred, label='Predicción', color='red', linestyle='dashed')
plt.legend()
plt.title('ARIMAX: Comparación de Predicciones vs Valores Reales')
plt.show()
print(model.summary())
# Librerías necesarias
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from statsmodels.stats.diagnostic import acorr_ljungbox

# Obtener los residuos del modelo
residuos = model.resid

# Configurar la figura
fig, axes = plt.subplots(2, 2, figsize=(12, 8))

# Gráfico de la serie temporal de los residuos
sns.lineplot(x=residuos.index, y=residuos, ax=axes[0, 0])
axes[0, 0].axhline(0, linestyle='--', color='red')
axes[0, 0].set_title('Residuos del Modelo ARIMAX')

# Histograma y curva de densidad de los residuos
sns.histplot(residuos, bins=20, kde=True, ax=axes[0, 1])
axes[0, 1].set_title('Distribución de los Residuos')

# Q-Q Plot (para ver si los residuos siguen una distribución normal)
stats.probplot(residuos, dist="norm", plot=axes[1, 0])
axes[1, 0].set_title('Q-Q Plot de los Residuos')

# Test de Ljung-Box para autocorrelación de residuos
lb_test = acorr_ljungbox(residuos, lags=[10], return_df=True)
p_value = lb_test["lb_pvalue"].values[0]

# Mostrar resultados del test de Ljung-Box
print(f"Test de Ljung-Box para autocorrelación de residuos:")
print(lb_test)

# Calcular la media de los errores
media_errores = residuos.mean()
# Aplicar la prueba de Ljung-Box para autocorrelación en los residuos
ljung_box = acorr_ljungbox(residuos, lags=[10], return_df=True)

# Crear una tabla con los resultados
resultados_df = pd.DataFrame({
    "Métrica": ["Media de los errores", "Ljung-Box p-valor (lag 10)"],
    "Valor": [media_errores, ljung_box["lb_pvalue"].values[0]]
})

# Mostrar la tabla de resultados
print("Resultados de la evaluación del modelo:")
display(resultados_df)


# Mostrar todos los gráficos
plt.tight_layout()
plt.show()
# Librerías necesarias
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

# Cargar datos
file_name = "/content/tabla_paro_ordenada.xlsx"
xls = pd.ExcelFile(file_name)
df = pd.read_excel(xls, sheet_name='TABLA')

# Convertir la columna 'fecha' a datetime y establecer como índice
df['fecha'] = pd.PeriodIndex(df['fecha'], freq='Q').to_timestamp()
df.set_index('fecha', inplace=True)

# Variables para el ARIMAX
y = df['tasa_paro']  # Variable dependiente (ORIGINAL, SIN DIFERENCIAR)
exog_vars = ['coste_laboral', 'empresas_creadas', 'salario_medio']  # Variables exógenas seleccionadas
X = df[exog_vars]  # Matriz de variables exógenas

# División de datos en entrenamiento (80%) y prueba (20%)
train_size = int(len(y) * 0.8)
y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]
X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]

# Definir el mejor modelo ARIMAX basado en la selección previa
best_order = (3, 1, 1)  # Ajustar con el mejor modelo encontrado

# Ajustar el modelo con los datos de entrenamiento
model = sm.tsa.ARIMA(y_train, order=best_order, exog=X_train).fit()

# Hacer predicciones sobre los datos de test
y_pred = model.predict(start=y_test.index[0], end=y_test.index[-1], exog=X_test)

# Evaluación del rendimiento del modelo
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mape = mean_absolute_percentage_error(y_test, y_pred) * 100

# Mostrar resultados
print(f"\nEvaluación del Modelo ARIMAX {best_order}:")
print(f"RMSE: {rmse:.2f}")
print(f"MAPE: {mape:.2f}%")

# Graficar los resultados en la escala original
plt.figure(figsize=(10,5))
plt.plot(y_train, label='Entrenamiento', color='blue')
plt.plot(y_test, label='Real (Test)', color='black')
plt.plot(y_pred, label='Predicción', color='red', linestyle='dashed')
plt.axvline(x=y_test.index[0], color='gray', linestyle='--', label='Inicio de Test')
plt.legend()
plt.title('ARIMAX: Comparación de Predicciones vs Valores Reales (Escala Original)')
plt.xlabel("Fecha")
plt.ylabel("Tasa de Paro")
plt.grid()
plt.show()

# Mostrar resumen del modelo
print(model.summary())
import pandas as pd
import numpy as np

# Cargar el archivo Excel
file_path = "/content/tabla_paro_ordenada.xlsx"
df = pd.read_excel(file_path)

# Aplicar transformaciones a las variables según lo solicitado
if 'tasa_paro' in df.columns:
    df['tasa_paro_transf'] = np.log(df['tasa_paro']).diff()
if 'Poblacion' in df.columns:
    df['Poblacion_transf'] = df['Poblacion'].diff().diff()
if 'PIB' in df.columns:
    df['PIB_transf'] = df['PIB']  # Sin diferencias
if 'Coste_laboral' in df.columns:
    df['Coste_laboral_transf'] = df['Coste_laboral'].diff().diff()
if 'IPC' in df.columns:
    df['IPC_transf'] = df['IPC'].diff()
if 'Empresas_creadas' in df.columns:
    df['Empresas_creadas_transf'] = df['Empresas_creadas'].diff().diff()
if 'Salario_medio' in df.columns:
    df['Salario_medio_transf'] = df['Salario_medio'].diff().diff()
if 'Contratos_indefinidos' in df.columns:
    df['Contratos_indefinidos_transf'] = df['Contratos_indefinidos'].diff().diff()

# Eliminar la primera o primeras filas con valores NaN generados por las diferencias
df_transformed = df.dropna()

# Guardar el archivo transformado
transformed_file_path = "/content/tabla_paro_transformada.xlsx"
df_transformed.to_excel(transformed_file_path, index=False)

# Mostrar el DataFrame transformado
display(df_transformed)
#3.SARIMA (2 diferencias)
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.tsa.statespace.sarimax import SARIMAX
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from google.colab import files

# Cargar el archivo desde Google Drive o localmente en Colab
uploaded = files.upload()
file_name = list(uploaded.keys())[0]
xls = pd.ExcelFile(file_name)
df = pd.read_excel(xls, sheet_name="TABLA")

# Convertir la columna 'fecha' en índice de serie temporal
df['fecha'] = pd.PeriodIndex(df['fecha'], freq='Q')
df.set_index('fecha', inplace=True)

# Extraer la serie de tasa de paro
serie_tasa_paro = df['tasa_paro']

# Aplicar el modelo ARIMA(1,2,2) x SARIMA(0,0,1,4)
modelo = SARIMAX(serie_tasa_paro, order=(1,2,2), seasonal_order=(0,0,1,4))
resultado = modelo.fit()

# Mostrar coeficientes del modelo y su significancia
print("Coeficientes del modelo:")
coeficientes = pd.DataFrame({
    'Coeficiente': resultado.params,
    'Error estándar': resultado.bse,
    'Valor z': resultado.tvalues,
    'p-valor': resultado.pvalues
})
print(coeficientes)

# Mostrar AIC y BIC
print("\nCriterios de información:")
criterios_info = pd.DataFrame({
    'AIC': [resultado.aic],
    'BIC': [resultado.bic]
})
print(criterios_info)

# Dividir en datos de entrenamiento y prueba
train_size = int(len(serie_tasa_paro) * 0.8)
train, test = serie_tasa_paro[:train_size], serie_tasa_paro[train_size:]

# Predicción en entrenamiento y prueba
pred_train = resultado.fittedvalues[:train_size]
pred_test = resultado.get_prediction(start=test.index[0], end=test.index[-1]).predicted_mean

# Calcular métricas RMSE y MAE
rmse_train = np.sqrt(mean_squared_error(train, pred_train))
rmse_test = np.sqrt(mean_squared_error(test, pred_test))
mae_train = mean_absolute_error(train, pred_train)
mae_test = mean_absolute_error(test, pred_test)

metricas = pd.DataFrame({
    'RMSE': [rmse_train, rmse_test],
    'MAE': [mae_train, mae_test]
}, index=['Entrenamiento', 'Prueba'])

print("\nMétricas de error:")
print(metricas)

# Análisis de residuos
residuos = resultado.resid
residuos.index = residuos.index.to_timestamp()  # Convertir PeriodIndex a Timestamp

plt.figure(figsize=(12,6))

plt.subplot(2,2,1)
plt.plot(residuos, label="Residuos", color="blue")
plt.axhline(y=0, color='black', linestyle='--')
plt.title("Residuos del modelo")
plt.legend()

plt.subplot(2,2,2)
plt.hist(residuos, bins=20, color="lightblue", edgecolor='black')
plt.title("Histograma de residuos")

sm.qqplot(residuos, line='s', fit=True)
plt.title("Gráfico QQ de residuos")

plt.subplot(2,2,4)
sm.graphics.tsa.plot_acf(residuos, lags=20)
plt.title("Autocorrelación de residuos")

plt.tight_layout()
plt.show()

# Predicción para 2020-2024
inicio_pred = '2020Q1'
fin_pred = '2024Q4'
predicciones = resultado.get_prediction(start=inicio_pred, end=fin_pred, dynamic=False)
pred_media = predicciones.predicted_mean

serie_tasa_paro.index = serie_tasa_paro.index.to_timestamp()
pred_media.index = pred_media.index.to_timestamp()

plt.figure(figsize=(12, 6))
plt.plot(serie_tasa_paro, label="Tasa de Paro Real", color="blue")
plt.plot(pred_media, label="Predicción ARIMA x SARIMA", color="red", linestyle="dashed")
plt.axvline(x=pd.Timestamp('2020-01-01'), color='black', linestyle='--', label='Inicio de Predicción')
plt.title("Comparación entre Predicción y Datos Reales (2020-2024)")
plt.xlabel("Fecha")
plt.ylabel("Tasa de Paro")
plt.legend()
plt.grid(True)
plt.show()

# Predicción para 2025-2026
predicciones_nuevas = resultado.get_forecast(steps=8)
pred_media_nueva = predicciones_nuevas.predicted_mean
pred_conf_int = predicciones_nuevas.conf_int()

pred_media_nueva.index = pred_media_nueva.index.to_timestamp()
pred_conf_int.index = pred_conf_int.index.to_timestamp()

plt.figure(figsize=(12, 6))
plt.plot(serie_tasa_paro, label="Tasa de Paro Real", color="blue")
plt.plot(pred_media_nueva, label="Predicción 2025-2026", color="green", linestyle="dashed")
plt.fill_between(pred_conf_int.index, pred_conf_int.iloc[:, 0], pred_conf_int.iloc[:, 1],
                 color='green', alpha=0.2, label="Intervalo de Confianza")
plt.axvline(x=pd.Timestamp('2025-01-01'), color='black', linestyle='--', label='Inicio de Predicción')
plt.title("Predicción de la Tasa de Paro para 2025-2026")
plt.xlabel("Fecha")
plt.ylabel("Tasa de Paro")
plt.legend()
plt.grid(True)
plt.show()

pred_df = pd.DataFrame({
    'Predicción': pred_media_nueva,
    'Límite Inferior': pred_conf_int.iloc[:, 0],
    'Límite Superior': pred_conf_int.iloc[:, 1]
})
print(pred_df)
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.tsa.statespace.sarimax import SARIMAX
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from statsmodels.stats.diagnostic import acorr_ljungbox
from scipy.stats import jarque_bera
from google.colab import files

# Cargar el archivo desde Google Drive o localmente en Colab
uploaded = files.upload()
file_name = list(uploaded.keys())[0]
xls = pd.ExcelFile(file_name)
df = pd.read_excel(xls, sheet_name="TABLA")

# Convertir la columna 'fecha' en índice de serie temporal
df['fecha'] = pd.PeriodIndex(df['fecha'], freq='Q')
df.set_index('fecha', inplace=True)

# Extraer la serie de tasa de paro
serie_tasa_paro = df['tasa_paro']

# Aplicar el modelo ARIMA(1,2,2) x SARIMA(0,0,1,4)
modelo = SARIMAX(serie_tasa_paro, order=(1,2,2), seasonal_order=(0,0,1,4))
resultado = modelo.fit()

# Mostrar coeficientes del modelo y su significancia
print("Coeficientes del modelo:")
coeficientes = pd.DataFrame({
    'Coeficiente': resultado.params,
    'Error estándar': resultado.bse,
    'Valor z': resultado.tvalues,
    'p-valor': resultado.pvalues
})
print(coeficientes)

# Mostrar AIC y BIC
print("\nCriterios de información:")
criterios_info = pd.DataFrame({
    'AIC': [resultado.aic],
    'BIC': [resultado.bic]
})
print(criterios_info)

# Dividir en datos de entrenamiento y prueba
train_size = int(len(serie_tasa_paro) * 0.8)
train, test = serie_tasa_paro[:train_size], serie_tasa_paro[train_size:]

# Predicción en entrenamiento y prueba
pred_train = resultado.fittedvalues[:train_size]
pred_test = resultado.get_prediction(start=test.index[0], end=test.index[-1]).predicted_mean

# Calcular métricas RMSE y MAE
rmse_train = np.sqrt(mean_squared_error(train, pred_train))
rmse_test = np.sqrt(mean_squared_error(test, pred_test))
mae_train = mean_absolute_error(train, pred_train)
mae_test = mean_absolute_error(test, pred_test)

metricas = pd.DataFrame({
    'RMSE': [rmse_train, rmse_test],
    'MAE': [mae_train, mae_test]
}, index=['Entrenamiento', 'Prueba'])

print("\nMétricas de error:")
print(metricas)

# Análisis de residuos
residuos = resultado.resid
residuos.index = residuos.index.to_timestamp()  # Convertir PeriodIndex a Timestamp

plt.figure(figsize=(12,6))

plt.subplot(2,2,1)
plt.plot(residuos, label="Residuos", color="blue")
plt.axhline(y=0, color='black', linestyle='--')
plt.title("Residuos del modelo")
plt.legend()

plt.subplot(2,2,2)
plt.hist(residuos, bins=20, color="lightblue", edgecolor='black')
plt.title("Histograma de residuos")

sm.qqplot(residuos, line='s', fit=True)
plt.title("Gráfico QQ de residuos")

plt.subplot(2,2,4)
sm.graphics.tsa.plot_acf(residuos, lags=20)
plt.title("Autocorrelación de residuos")

plt.tight_layout()
plt.show()

# Test de normalidad y autocorrelación
jb_test = jarque_bera(residuos)
ljung_box_test = acorr_ljungbox(residuos, lags=[10], return_df=True)

# Crear tabla con resultados de Jarque-Bera y Ljung-Box
test_residuos = pd.DataFrame({
    "Estadístico Jarque-Bera": [jb_test[0]],
    "p-valor Jarque-Bera": [jb_test[1]],
    "Estadístico Ljung-Box": ljung_box_test['lb_stat'].values,
    "p-valor Ljung-Box": ljung_box_test['lb_pvalue'].values
})
print("\nResultados de pruebas estadísticas sobre residuos:")
print(test_residuos)

# Predicción para 2020-2024
inicio_pred = '2020Q1'
fin_pred = '2024Q4'
predicciones = resultado.get_prediction(start=inicio_pred, end=fin_pred, dynamic=False)
pred_media = predicciones.predicted_mean

serie_tasa_paro.index = serie_tasa_paro.index.to_timestamp()
pred_media.index = pred_media.index.to_timestamp()

plt.figure(figsize=(12, 6))
plt.plot(serie_tasa_paro, label="Tasa de Paro Real", color="blue")
plt.plot(pred_media, label="Predicción ARIMA x SARIMA", color="red", linestyle="dashed")
plt.axvline(x=pd.Timestamp('2020-01-01'), color='black', linestyle='--', label='Inicio de Predicción')
plt.title("Comparación entre Predicción y Datos Reales (2020-2024)")
plt.xlabel("Fecha")
plt.ylabel("Tasa de Paro")
plt.legend()
plt.grid(True)
plt.show()

# Predicción para 2025-2026
predicciones_nuevas = resultado.get_forecast(steps=8)
pred_media_nueva = predicciones_nuevas.predicted_mean
pred_conf_int = predicciones_nuevas.conf_int()

pred_media_nueva.index = pred_media_nueva.index.to_timestamp()
pred_conf_int.index = pred_conf_int.index.to_timestamp()

plt.figure(figsize=(12, 6))
plt.plot(serie_tasa_paro, label="Tasa de Paro Real", color="blue")
plt.plot(pred_media_nueva, label="Predicción 2025-2026", color="green", linestyle="dashed")
plt.fill_between(pred_conf_int.index, pred_conf_int.iloc[:, 0], pred_conf_int.iloc[:, 1],
                 color='green', alpha=0.2, label="Intervalo de Confianza")
plt.axvline(x=pd.Timestamp('2025-01-01'), color='black', linestyle='--', label='Inicio de Predicción')
plt.title("Predicción de la Tasa de Paro para 2025-2026")
plt.xlabel("Fecha")
plt.ylabel("Tasa de Paro")
plt.legend()
plt.grid(True)
plt.show()

pred_df = pd.DataFrame({
    'Predicción': pred_media_nueva,
    'Límite Inferior': pred_conf_int.iloc[:, 0],
    'Límite Superior': pred_conf_int.iloc[:, 1]
})
print(pred_df)
#ARIMAX(LOG)
# Librerías necesarias
import pandas as pd
import numpy as np
import itertools
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
import warnings
warnings.filterwarnings("ignore")

# Cargar datos
file_name = "/content/tabla_paro_transformada.xlsx"
xls = pd.ExcelFile(file_name)
df = pd.read_excel(xls, sheet_name='Sheet1')

# Convertir la columna 'fecha' a datetime y establecer como índice
df['fecha'] = pd.PeriodIndex(df['fecha'], freq='Q').to_timestamp()
df.set_index('fecha', inplace=True)

# Variables para el ARIMAX
y = df['tasa_paro']  # Variable dependiente
exog_vars = ['coste_laboral', 'empresas_creadas', 'salario_medio', 'PIB']  # Variables exógenas seleccionadas
X = df[exog_vars]  # Matriz de variables exógenas

# Comprobación de estacionariedad
result = adfuller(y)
if result[1] > 0.05:  # Si p-valor > 0.05, diferenciamos para hacerlo estacionario
    y = y.diff().dropna()
    X = X.iloc[1:]  # Ajustar X para que tenga la misma cantidad de datos que y diferenciada

# Definir combinaciones de parámetros ARIMA (p, d, q)
p = range(0, 3)  # Autoregresivos
d = range(0, 2)  # Diferenciación
q = range(0, 3)  # Media móvil

# Generar todas las combinaciones posibles
param_combinations = list(itertools.product(p, d, q))

# Ajustar modelos ARIMAX y encontrar el mejor (según AIC)
best_aic = float("inf")
best_order = None
best_model = None

for order in param_combinations:
    try:
        model = sm.tsa.ARIMA(y, order=order, exog=X).fit()
        aic = model.aic
        if aic < best_aic:
            best_aic = aic
            best_order = order
            best_model = model
    except:
        continue

# Mostrar el mejor modelo
print(f"\nMejor modelo ARIMAX encontrado: ARIMA{best_order} con AIC = {best_aic:.2f}")
print(best_model.summary())
#sarima con log
# Instalar dependencias en Google Colab
!pip install statsmodels openpyxl

# Importar librerías necesarias
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.tsa.statespace.sarimax import SARIMAX
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error
from google.colab import files

# Cargar el archivo desde Google Drive o localmente en Colab
uploaded = files.upload()
file_name = list(uploaded.keys())[0]
xls = pd.ExcelFile(file_name)
df = pd.read_excel(xls, sheet_name="TABLA")

# Convertir la columna 'fecha' en índice de serie temporal
df['fecha'] = pd.PeriodIndex(df['fecha'], freq='Q')
df.set_index('fecha', inplace=True)

# Extraer la serie de tasa de paro y aplicar logaritmo
serie_original = df['tasa_paro']
serie_tasa_paro = np.log(serie_original)  # Aplicar logaritmo para estabilizar la varianza

# Ajustar el modelo SARIMA
modelo = SARIMAX(serie_tasa_paro, order=(1,1,0), seasonal_order=(1,0,1,4))
resultado = modelo.fit()

# Mostrar coeficientes del modelo y su significancia
print("Coeficientes del modelo:")
coeficientes = pd.DataFrame({
    'Coeficiente': resultado.params,
    'Error estándar': resultado.bse,
    'Valor z': resultado.tvalues,
    'p-valor': resultado.pvalues
})
print(coeficientes)

# Mostrar AIC y BIC
print("\nCriterios de información:")
criterios_info = pd.DataFrame({
    'AIC': [resultado.aic],
    'BIC': [resultado.bic]
})
print(criterios_info)

# Dividir en datos de entrenamiento y prueba
train_size = int(len(serie_tasa_paro) * 0.8)
train, test = serie_tasa_paro[:train_size], serie_tasa_paro[train_size:]

# Predicción en entrenamiento y prueba (convertir de log a original)
pred_train = np.exp(resultado.fittedvalues[:train_size])
pred_test = np.exp(resultado.get_prediction(start=test.index[0], end=test.index[-1]).predicted_mean)

# Calcular métricas RMSE y MAE sobre la serie original
rmse_train = np.sqrt(mean_squared_error(np.exp(train), pred_train))
rmse_test = np.sqrt(mean_squared_error(np.exp(test), pred_test))
mae_train = mean_absolute_error(np.exp(train), pred_train)
mae_test = mean_absolute_error(np.exp(test), pred_test)

metricas = pd.DataFrame({
    'RMSE': [rmse_train, rmse_test],
    'MAE': [mae_train, mae_test]
}, index=['Entrenamiento', 'Prueba'])

print("\nMétricas de error en la serie original:")
print(metricas)

# Predicción para 2020-2024 sobre la serie original
inicio_pred = '2020Q1'
fin_pred = '2024Q4'
predicciones = resultado.get_prediction(start=inicio_pred, end=fin_pred, dynamic=False)
pred_media = np.exp(predicciones.predicted_mean)  # Transformar de logaritmo a la escala original

# Graficar la serie original con predicciones
serie_original.index = serie_original.index.to_timestamp()
pred_media.index = pred_media.index.to_timestamp()

plt.figure(figsize=(12, 6))
plt.plot(serie_original, label="Tasa de Paro Real", color="blue")
plt.plot(pred_media, label="Predicción ARIMA x SARIMA", color="red", linestyle="dashed")
plt.axvline(x=pd.Timestamp('2020-01-01'), color='black', linestyle='--', label='Inicio de Predicción')
plt.title("Comparación entre Predicción y Datos Reales (2020-2024)")
plt.xlabel("Fecha")
plt.ylabel("Tasa de Paro")
plt.legend()
plt.grid(True)
plt.show()

# Predicción para 2025-2026 en la serie original
predicciones_nuevas = resultado.get_forecast(steps=8)
pred_media_nueva = np.exp(predicciones_nuevas.predicted_mean)  # Transformación inversa de logaritmo
pred_conf_int = np.exp(predicciones_nuevas.conf_int())  # Intervalo de confianza en la escala original

pred_media_nueva.index = pred_media_nueva.index.to_timestamp()
pred_conf_int.index = pred_conf_int.index.to_timestamp()

plt.figure(figsize=(12, 6))
plt.plot(serie_original, label="Tasa de Paro Real", color="blue")
plt.plot(pred_media_nueva, label="Predicción 2025-2026", color="green", linestyle="dashed")
plt.fill_between(pred_conf_int.index, pred_conf_int.iloc[:, 0], pred_conf_int.iloc[:, 1],
                 color='green', alpha=0.2, label="Intervalo de Confianza")
plt.axvline(x=pd.Timestamp('2025-01-01'), color='black', linestyle='--', label='Inicio de Predicción')
plt.title("Predicción de la Tasa de Paro para 2025-2026")
plt.xlabel("Fecha")
plt.ylabel("Tasa de Paro")
plt.legend()
plt.grid(True)
plt.show()

# Crear tabla con predicciones para 2025-2026
pred_df = pd.DataFrame({
    'Predicción': pred_media_nueva,
    'Límite Inferior': pred_conf_int.iloc[:, 0],
    'Límite Superior': pred_conf_int.iloc[:, 1]
})
print("\nPredicciones para 2025-2026 sobre la serie original:")
print(pred_df)

# Análisis de residuos
residuos = resultado.resid
residuos.index = residuos.index.to_timestamp()  # Convertir PeriodIndex a Timestamp

plt.figure(figsize=(12,6))

plt.subplot(2,2,1)
plt.plot(residuos, label="Residuos", color="blue")
plt.axhline(y=0, color='black', linestyle='--')
plt.title("Residuos del modelo")
plt.legend()

plt.subplot(2,2,2)
plt.hist(residuos, bins=20, color="lightblue", edgecolor='black')
plt.title("Histograma de residuos")

sm.qqplot(residuos, line='s', fit=True)
plt.title("Gráfico QQ de residuos")

plt.subplot(2,2,4)
sm.graphics.tsa.plot_acf(residuos, lags=20)
plt.title("Autocorrelación de residuos")

plt.tight_layout()
plt.show()
# Instalar dependencias necesarias en Google Colab
!pip install statsmodels

# Importar librerías necesarias
import numpy as np
import statsmodels.api as sm
import statsmodels.stats.diagnostic as diag
import pandas as pd

# Obtener los residuos del modelo
residuos = resultado.resid

# Test de Jarque-Bera (normalidad de los residuos)
jb_test = sm.stats.jarque_bera(residuos)
jb_resultado = pd.DataFrame({
    "Estadístico JB": [jb_test[0]],
    "p-valor": [jb_test[1]],
    "Curtosis": [jb_test[2]],
    "Asimetría": [jb_test[3]]
})
print("Test de Jarque-Bera sobre los residuos:")
print(jb_resultado)

# Test de Ljung-Box (independencia de los residuos)
ljung_box_test = diag.acorr_ljungbox(residuos, lags=[10], return_df=True)
print("\nTest de Ljung-Box sobre los residuos:")
print(ljung_box_test)

# Media de los errores (residuos)
media_errores = np.mean(residuos)
print("\nMedia de los errores (residuos):", media_errores)

